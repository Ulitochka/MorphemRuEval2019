### 95 acc

NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(60, 500, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(62, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.3)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3)
      (layers): ModuleList(
        (0): LSTMCell(1000, 500)
        (1): LSTMCell(500, 500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=500, out_features=500, bias=False)
      (linear_out): Linear(in_features=1000, out_features=500, bias=False)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=500, out_features=62, bias=True)
    (1): LogSoftmax()
  )
)


batch_size=64,
batch_type='sents',
bridge=False,
brnn=False,
cnn_kernel_width=3,
config=None,
context_gate=None,
copy_attn=False,
copy_attn_force=False,
copy_loss_by_seqlength=False,
coverage_attn=False,
dec_layers=2,
dec_rnn_size=500,
decay_method='none',
decay_steps=10000,
decoder_type='rnn',
dropout=0.3,
enc_layers=2,
enc_rnn_size=500,
encoder_type='rnn',
feat_vec_exponent=0.7,
feat_vec_size=-1,
fix_word_vecs_dec=False,
fix_word_vecs_enc=False,
generator_function='softmax',
global_attention='general',
global_attention_function='softmax',
learning_rate=1.0,
learning_rate_decay=0.5,
max_generator_batches=32,
max_grad_norm=5,
model_type='text',
normalization='sents',
optim='sgd',
position_encoding=False,
rnn_type='LSTM',
src_word_vec_size=500,
start_decay_steps=50000,
tgt_word_vec_size=500




